{
 "metadata": {
  "name": "two-phase-threaded"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Interactive monitoring of a shallow water wave parallel simulation with Proteus and IPython"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Brian E. Granger(1), Chris E. Kees(2), Fernando Perez(3), Benjamin Ragan-Kelley(3) and Jose Unpingco (4).\n",
      "\n",
      "(1) California Polytechnic State University, San Luis Obispo, CA.\n",
      "\n",
      "(2) Army Engineer Research and Development Center, Vicksburg, MS.\n",
      "\n",
      "(3) University of California, Berkeley, CA.\n",
      "\n",
      "(4) DRC Inc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this technical report, we will illustrate how the IPython parallel computing capabilities can be combined with the interactive IPython notebook, to introspect and monitor the evolution of an existing HPC code that uses MPI for its core communication needs.  \n",
      "\n",
      "The [Proteus toolkit](http://proteus.usace.army.mil) is a modeling package for the numerical simulation of continuous processes such as the shallow water wave equations.  Proteus was designed to run in traditional HPC environments, using the MPI libraries for efficient inter-node communication.  The [IPython project](http://ipython.org) provides tools for interactive computing that include a web-based interface called the IPython Notebook and a set of high-level libraries for parallel computing that complement MPI.\n",
      "\n",
      "We will construct a simulation with Proteus that will run in any traditional HPC environment and will monitor its evolution interactively from the web notebook, using the notebook's facilities for data visualization.  The interactive monitoring will be performed without stopping or slowing down the parallel simulation.\n",
      "\n",
      "This will be accomplished without requiring any changes to the architecture of Proteus and only a trivial, minimal change to how the code would have been executed in batch mode.  Importantly, this change does not introduce an IPython dependency into the simulation and has no discernible performance impact. This allows for the same code to be used in production runs absent interactive monitoring."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load IPython support for working with MPI tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The rest of this notebook assumes that you have already started an IPython cluster with the `mpi` profile.  If you have not done so yet, use [the cluster tab in the Dashboard](/#tab2) to start your `mpi` cluster.  If you do not have a cluster configuration for MPI use, see the IPython documentation for [detailed instructions](http://ipython.org/ipython-doc/rel-0.12.1/parallel/parallel_process.html#using-ipcluster-in-mpiexec-mpirun-mode) on how to create one.\n",
      "\n",
      "We begin by creating a cluster client that gives us a local handle on the engines running in the (possibly remote) MPI cluster.  From the client we make a `view` object, which we set to use blocking mode by default as it is more convenient for interactive control.  Since the real computation will be done over MPI without IPython intervention, setting the default behavior to be blocking will have no significant performance impact:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client, error\n",
      "cluster = Client(profile=\"mpi\")\n",
      "view = cluster[:]\n",
      "view.block = True\n",
      "%load_ext parallelmagic\n",
      "view.activate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we load the MPI libraries into the engine namespaces, and do a simple printing of their MPI rank information to verify that all nodes are operational and they match our cluster's real capacity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "from mpi4py import MPI\n",
      "mpi = MPI.COMM_WORLD\n",
      "bcast = mpi.bcast\n",
      "barrier = mpi.barrier\n",
      "rank = mpi.rank\n",
      "print \"MPI rank: %i/%i\" % (mpi.rank,mpi.size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:0] MPI rank: 2/4\n",
        "[stdout:1] MPI rank: 3/4\n",
        "[stdout:2] MPI rank: 1/4\n",
        "[stdout:3] MPI rank: 0/4\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Proteus libraries and set simulation parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now create the Proteus objects that parametrize our example, except for the main simulation object that will be created later.  That will make it more convenient to recreate a fresh one by only re-executing a single cell without re-defining all parameters.\n",
      "\n",
      "In this example, we will use Proteus to solve the shallow water equations for cylindrically symmetric flow impacting the center of a flat container with square shape and perfectly reflecting walls:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "# Required imports\n",
      "from proteus.iproteus import * \n",
      "from proteus import default_n, default_s, default_so\n",
      "import wavetank\n",
      "he = wavetank.L[2]/10.0\n",
      "wavetank.triangleOptions=\"VApq2q10ena%21.16e\" % ((he**3)/6.0,)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "pList = []\n",
      "nList = []\n",
      "import wavetank_so\n",
      "so = wavetank_so\n",
      "so.sList=[]\n",
      "for p,n in wavetank_so.pnList:\n",
      "    pList.append(__import__(p))\n",
      "    nList.append(__import__(n))\n",
      "    pList[-1].name = p\n",
      "    so.sList.append(default_s)\n",
      "so.name='wavetank'\n",
      "#opts.viewer='vtk'\n",
      "#viewers = Viewers.viewerOn(so.name+`comm.rank()`,opts.viewer)\n",
      "#ns = NumericalSolution.NS_base(so,pList,nList,so.sList,opts)\n",
      "#ns.calculateSolution('run1')\n",
      "#x = ns.modelList[0].levelModelList[-1].mesh.nodeArray[:,0]\n",
      "#y = ns.modelList[0].levelModelList[-1].mesh.nodeArray[:,1]\n",
      "#z = ns.modelList[0].levelModelList[-1].mesh.nodeArray[:,2]\n",
      "#triangles = ns.modelList[0].levelModelList[-1].mesh.elementBoundaryNodesArray\n",
      "#p = ns.modelList[0].levelModelList[-1].u[0].dof\n",
      "#u = ns.modelList[0].levelModelList[-1].u[1].dof\n",
      "#v = ns.modelList[0].levelModelList[-1].u[2].dof\n",
      "#phi = ns.modelList[1].levelModelList[-1].u[0].dof\n",
      "#vof = ns.modelList[2].levelModelList[-1].u[0].dof\n",
      "#if comm.isMaster():\n",
      "#    imageList = [window.png for window in viewers.windowDict.values()]\n",
      "#else:\n",
      "#    imageList = []\n",
      "##Combine subdomain solutions and plot\n",
      "from petsc4py import PETSc\n",
      "\n",
      "for n in nList:\n",
      "    n.multilevelLinearSolver = default_n.KSP_petsc4py\n",
      "\n",
      "# PETSc solver configuration\n",
      "OptDB = PETSc.Options()\n",
      "OptDB.setValue(\"ksp_type\", \"preonly\")\n",
      "OptDB.setValue(\"pc_type\", \"lu\")\n",
      "OptDB.setValue(\"pc_factor_mat_solver_package\", \"superlu_dist\")\n",
      "#OptDB.setValue(\"sub_pc_type\", \"jacobi\")\n",
      "OptDB.setValue(\"ksp_rtol\", 0.0)\n",
      "OptDB.setValue(\"ksp_atol\", 1.0e-8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Making a simulation object that can be monitored interactively"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we create a numerical simulation object, `ns`, that we will use to compute the desired solution.  The key idea for performing interactive monitoring of an ongoing MPI simulation is to call the main solution method (in this case `ns.calculateSolution(...)`) in a *secondary worker thread*, leaving the main thread available to answer for IPython requests.  This separation allows all computation and MPI communications to happen in one thread, while the other is available for communication with an interactive control client (typically, but not necessarily, a human inspecting the results).  While we have chosen to put the IPython code in the main thread and the MPI code in the secondary one, these roles can be reveresed without ill effects.\n",
      "\n",
      "In cases where the numerical code has manual control of the time-stepping, one can very easily write a small helper class that advances the simulation step by step from the thread.  Such a configuration will then permit not only monitorig and inspection, but also pausing and restarting of the simulation (even with adjustment of parameters).  But for now, the following simple code suffices that uses the default `Thread` class without modifications.\n",
      "\n",
      "Note: we run this cell in non-blocking mode only to silence a lot of verbose terminal output from all nodes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --noblock\n",
      "from threading import Thread\n",
      "\n",
      "# Create simulation object (but don't start the computation yet)\n",
      "ns = NumericalSolution.NS_base(so, pList, nList, so.sList, opts)\n",
      "\n",
      "# Create a thread wrapper for the simulation.  The target must be an argument-less\n",
      "# function so we wrap the call to `calculateSolution` in a simple lambda:\n",
      "simulation_thread = Thread(target = lambda : ns.calculateSolution('run1'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<AsyncResult: execute>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now define a local (to this notebook) plotting function that makes a remote call in the engines to populate some variables in the global namespace.  Once it has retrieved the current state of the relevant variables, it produces and returns a figure:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from IPython.core.display import clear_output\n",
      "\n",
      "def plot_current_results(in_place=True):\n",
      "    \"\"\"Makes a blocking call to retrieve remote data and displays the solution mesh\n",
      "    as a contour plot.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    in_place : bool\n",
      "        By default it calls clear_output so that new plots replace old ones.  Set\n",
      "        to False to allow keeping of all previous outputs.\n",
      "    \"\"\"\n",
      "    \n",
      "    # We make a blocking call to load the remote data from the simulation into simple named \n",
      "    # variables we can read from the engine namespaces\n",
      "    view.apply_sync(load_simulation_globals)\n",
      "    # And now we can use the view to read these variables from all the engines.  Then we\n",
      "    # concatenate all of them into single arrays for local plotting\n",
      "    x = np.concatenate(view['x'])\n",
      "    z = np.concatenate(view['z'])\n",
      "    p = np.concatenate(view['p'])\n",
      "    u = np.concatenate(view['u'])\n",
      "    v = np.concatenate(view['v'])\n",
      "    phi = np.concatenate(view['phi'])\n",
      "    vf = np.concatenate(view['vf'])\n",
      "    shifts = numpy.cumsum([0]+view['nn'][:-1])\n",
      "    flat_triangles = np.concatenate([ tri + shift for tri,shift in zip(view['triangles'], shifts) ])\n",
      "    # We can now call the matplotlib plotting function we need\n",
      "    fig, ax = plt.subplots(subplot_kw=dict(aspect='equal'))\n",
      "    ax.tricontourf(x, z, flat_triangles,vf)\n",
      "    # We clear the notebook output before plotting this if in-place plot updating is requested\n",
      "    if in_place:\n",
      "        clear_output()\n",
      "    display(fig)\n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the function that will be called *on the engines* by the plotting code to load the relevant data structures into named variables in the global namespace that we can just read off:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def load_simulation_globals():\n",
      "    \"\"\"Put some variables we need in engine namespace.\n",
      "\n",
      "    These can then be retrieved by clients for inspection, visualization, etc.\n",
      "    \"\"\"\n",
      "    global nn,x,z,p,u,v,phi,vf,triangles\n",
      "    model_nse = ns.modelList[0].levelModelList[-1]\n",
      "    model_ls = ns.modelList[1].levelModelList[-1]\n",
      "    model_vf = ns.modelList[2].levelModelList[-1]\n",
      "    nodes = model_nse.mesh.nodeArray\n",
      "    triangles = model_nse.mesh.elementBoundaryNodesArray\n",
      "    x = nodes[:,0]\n",
      "    z = nodes[:,2]\n",
      "    p = model_nse.u[0].dof\n",
      "    u = model_nse.u[1].dof\n",
      "    v = model_nse.u[2].dof\n",
      "    phi = model_ls.u[0].dof\n",
      "    vf  = model_vf.u[0].dof\n",
      "    nn = len(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also define a utility to check whether any of our engines is still busy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def simulation_alive():\n",
      "    \"\"\"Return True if the simulation thread is still running on any engine.\n",
      "    \"\"\"\n",
      "    return any(view.apply_sync(lambda : simulation_thread.is_alive()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, this is a convenience wrapper around the plotting code so that we can interrupt monitoring at any point, and that will provide basic timing information:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def monitor_simulation(plots_in_place=True):\n",
      "    \"\"\"Monitor the simulation progress and call plotting routine.\n",
      "\n",
      "    Supress KeyboardInterrupt exception if interrupted, ensure that the last \n",
      "    figure is always displayed and provide basic timing and simulation status.\n",
      "    \"\"\"\n",
      "    import datetime as dt\n",
      "    \n",
      "    if not simulation_alive():\n",
      "        plot_current_results(in_place=plots_in_place)\n",
      "        plt.close('all')\n",
      "        print 'Simulation has already finished, no monitoring to do.'\n",
      "        return\n",
      "    \n",
      "    t0 = dt.datetime.now()\n",
      "    fig = None\n",
      "    try:\n",
      "        while simulation_alive():\n",
      "            fig = plot_current_results(in_place=plots_in_place)\n",
      "            plt.close('all') # prevent re-plot of old figures\n",
      "    except (KeyboardInterrupt, error.TimeoutError):\n",
      "        msg = 'Monitoring interrupted, simulation is ongoing!'\n",
      "    else:\n",
      "        msg = 'Simulation completed!'\n",
      "    tmon = dt.datetime.now() - t0\n",
      "    if plots_in_place and fig is not None:\n",
      "        clear_output()\n",
      "        display(fig)\n",
      "    print msg\n",
      "    print 'Monitored for: %s.' % tmon"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Start simulation and monitor its evolution interactively"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start the simulation by calling the `start()` method of our thread wrapper on all engines.  They will then run all the communications related to the numerical computation over MPI, while listening on the IPython sockets for data requests:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px simulation_thread.start()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The monitoring cell below can be interrupted any time by using the `Kernel Interrupt` menu option, the 'stop' icon in the toolbar or the interrupt keybinding (`Control-m-i`).  It can be interrupted and restarted as many times as desired, as it will simply read and display the currently available data.  Calling the function with `plots_in_place` set to `False` will keep the entire sequence of plots visible in the output area."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "monitor_simulation()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABhCAYAAADLESqIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADX1JREFUeJzt3X9sE3UfB/B3B1vCDxkMZMK2ZEAr2zLWNhmpk0xlipLJ\nhjKNmzGQwYYhIcuMJjMkTyI+CTCVRBPM84hRDKJz/Lep5VSEORzuh1KZCQaHQlwnmSk4N5nSUe75\nY0+3tmuvd73erl3fr3/Yet/7fj/3vbvPltv3PhhEURRBRERxK0nvAIiISB0mciKiOMdETkQU55jI\niYjiHBM5EVGcYyInIopzkol8+/btSE9Px5o1a0K2qaurg8lkgtlshsPhiHqAREQkTTKRV1dXQxCE\nkNvtdjsuXbqEvr4+HD58GLt27Yp6gEREJE0ykRcXF2PRokUht7e2tmLbtm0AAJvNhqGhIQwODkY3\nQiIikjRbzc4DAwPIysqa+D4zMxNOpxPp6el+7QwGg5phiIgSlpyX71Ul8mCDhEra59UONEP8BwAf\nQI3jXEziXEziXEwyy2ynatVKRkYG+vv7J753Op3IyMhQ0yURESmkKpGXl5fj6NGjAIDOzk4sXLhw\nymMVIiLSluSjlaqqKnz11VdwuVzIysrC3r17MTY2BgB49tlnUVpaCrvdDqPRiHnz5uHIkSPTEnQ8\nK9Q7gBjCuZjEuZjEuVDOMB1lbA0GA5+RExEpZIa8P3byzU4iojjHRE5EFOeYyImI4hwTORFRnAub\nyAVBQE5ODkwmExobG6dsd7lc2LhxIywWC/Lz8/Hee+9pEScREYUguWrF4/Fg9erVOHnyJDIyMrB2\n7Vo0NTUhNzd3os1LL72EmzdvYv/+/XC5XFi9ejUGBwcxe/bkykauWiEiUi4qq1a6u7thNBqRnZ2N\n5ORkVFZWoqWlxa/NsmXLMDw8DAAYHh7G4sWL/ZI4ERFpSzLjBiuK1dXV5demtrYWJSUlWL58OUZG\nRnD8+PGgff3H5+tCAGsjDpmIaGbqAfBtBPtJJnI5VQv37dsHi8WCtrY2/Pzzz9iwYQPOnz+PO+64\nw68di+AQEUlbC/9fcv8rcz/JRyuBRbH6+/uRmZnp1+bs2bN48sknAQCrVq3CihUrcPHiRZnDExGR\nWpKJvLCwEH19fbhy5Qrcbjeam5tRXl7u1yYnJwcnT54EAAwODuLixYtYuXKldhETEZEfyUcrs2fP\nxqFDh/DII4/A4/Fgx44dyM3NxVtvvQVgvHDWnj17UF1dDbPZjNu3b+OVV15BWlratARPREQsmkVE\nFLNYNIuIKEEwkRMRxTkmciKiOMdETkQU51QXzQKAtrY2WK1W5Ofn44EHHoh2jEREJEF10ayhoSGs\nW7cOn332GTIzM+FyubBkyRL/QbhqhYhIsWkrmvXhhx+ioqJi4o3PwCRORETakkzkwYpmDQwM+LXp\n6+vD9evXsX79ehQWFuL999/XJlIiIgpKddGssbExnDt3Dl9++SVGR0dRVFSEe+65ByaTya8dqx8S\nEUnTpPqhnKJZWVlZWLJkCebMmYM5c+bgvvvuw/nz56ckclY/JCKSpkn1QzlFszZv3oyvv/4aHo8H\no6Oj6OrqQl5enrLoiYgoYqqLZuXk5GDjxo0oKChAUlISamtrmchJEwXPA70H9Y5CvoLnx/+Np5ij\nxXvsQGIe/3SLmaJZWp943/7VjBOsHzX96UHqGIDoHYfaxBvrcx14fNGYV98+p/M8heszMBapsaN1\nr2lNi5wT7T7lLj+ctkQuhrkotTbcmIIFDW59g6CIDTemTHwd6+fRN1Yg9uPVinceEvX4o8FwMMYS\n+Z+3UsI3DLCgwT3lptCa2jH1iDmUWIqFiJRLne2OrUT+L3GPon1e8Lw28fVrs16IWizefgP79B0v\n0jG1ijkS0TgerYQ6B5H2JdVPuO1ajj3TSF1T0TynWoul+9RXsOvp34Z9sZXIU679OfF9Q9pkzZbG\n6w1+nzVeb/Db7svb1ldDWmPQz4ORGlfuWHK3S7XTWqhjAtTH43ue1OwfaR/e8x14jIF9hdsudxyp\n7d5+lVyDUv1EM75okrqeAulxvfsKnBc5sUeSP5Tuq6Rv3z7di1NjN5Gr4d69YOLrlEPDQT8Ptl0L\noWJRsp+vUH2Eah9u32BJQu48hWsnJyY5cSqdi1DknItwMaccGoZ794KQx6nmepIz70rmVM21Emp/\nOTGG+8Hj7UPNsYWLIVSfwcaUc84ivb+U7B+sv1Bz5dd3tBK5IAior6+Hx+NBTU0NGhqCn8Cenh4U\nFRXh+PHj2LJli/8gBgNQJR2M74GF1dQ7/m9VgbzPFVAURwyTe6Oq4p1vL995j8K5COQ9pplwfsLy\nnVvvHAb7TGl/EZ6PSK+nKfdT4DUTjESM4a4BOfdv0DZSccmds2Bz3NSr7h5oMqhP5HKqH3rbbdiw\nAXPnzkV1dTUqKir8BzEYgFALEEMlY+82qe/DCZZYpMYOFYcSvjEG61/pzajywo+oP6k+I50XOf2H\n6jvc8UUrJqVCHYOSY9NifLX7R/pDQqrvaBx/uFwRLq5w+UDp+JH0pTgGs/pE/s0332Dv3r0QBAEA\ncODAAQDAiy++6Nfu9ddfR0pKCnp6erBp0yZliRxQnqBJvmC/zU3n/tGi9iYk9ZQmZy2uHak+Z+Q1\nIi+RS77ZGaz6YVdX15Q2LS0tOHXqFHp6eiQKbUmUzZoxkx6D1M5trJybWIkjkSk9B1qcM6k+Z8Q1\nElnZLNXVD+vr63HgwIHxl35EUeKnB8tmERFJi6xslurqh9999x0qKysBAC6XCydOnEBycvKU4lpE\nRKQNyUTuW/1w+fLlaG5uRlNTk1+bX375ZeLr6upqlJWVMYkTEU0j1dUPiYhIX9P2QpDkqhUiIgpC\n3qoVyf9YgoiIYh8TORFRnGMiJyKKc0zkRERxLmwiFwQBOTk5MJlMaGycWsbxgw8+gNlsRkFBAdat\nW4fe3pnwdhURUfyQXH7o8Xiwe/duv6JZ5eXlfkWzVq5cifb2dqSmpkIQBOzcuROdnZ2aB05EROMk\nfyPv7u6G0WhEdnY2kpOTUVlZiZaWFr82RUVFSE1NBQDYbDY4nU7toiUioilUF83y9c4776C0tDTE\nVomiWUREBN2KZnmdPn0a7777Ljo6OkK0YNEsIiJpOhXNAoDe3l7U1tZCEAQsWrRI1sBERBQdks/I\nfYtmud1uNDc3TymI9euvv2LLli04duwYjEajpsESEdFUqotmvfzyy/jjjz+wa9f4o5Pk5GR0d3dr\nHzkREQFg0SwiohjGollERAmBiZyIKM4xkRMRxTkmciKiOMdEPu169A4ghnAuJnEuJnEulFJd/RAA\n6urqYDKZYDab4XA4oh7kzKL89duZi3MxiXMxiXOhlGQi91Y/FAQBFy5cQFNTE3788Ue/Nna7HZcu\nXUJfXx8OHz48sZ6ciIimh+rqh62trdi2bRuA8eqHQ0NDGBwc1C5iIiLyo7r6YbA2TqcT6enpAb2Z\n1Uc7Y8grhJMYOBeTOBeTOBdKRKX6YeCbR4H7TcPLo0RECUvy0Yqc6oeBbZxOJzIyMqIcJhERhaK6\n+mF5eTmOHj0KAOjs7MTChQuDPFYhIiKtqK5+WFpaCrvdDqPRiHnz5uHIkSPTEjgREY3TvPqhIAio\nr6+Hx+NBTU0NGhoatBwuZm3fvh2ffvopli5dih9++EHvcHTV39+PrVu34vfff4fBYMDOnTtRV1en\nd1i6+Oeff3D//ffj5s2bcLvd2Lx5M/bv3693WLryeDwoLCxEZmYmPv74Y73D0U12djYWLFiAWbNm\nhS8PLmro1q1b4qpVq8TLly+LbrdbNJvN4oULF7QcMma1t7eL586dE/Pz8/UORXdXr14VHQ6HKIqi\nODIyIt59990Je12IoijeuHFDFEVRHBsbE202m3jmzBmdI9LXwYMHxaefflosKyvTOxRdZWdni9eu\nXZPVVtNX9OWsQ08UxcXF/G/w/u+uu+6CxWIBAMyfPx+5ubn47bffdI5KP3PnzgUAuN1ueDwepKWl\n6RyRfpxOJ+x2O2pqarjaDfJX/GmayIOtMR8YGNBySIozV65cgcPhgM1m0zsU3dy+fRsWiwXp6elY\nv3498vLy9A5JN8899xxeffVVJCWxDJTBYMBDDz2EwsJCvP3225JtNZ0tuevQKTH99ddfeOKJJ/DG\nG29g/vz5eoejm6SkJHz//fdwOp1ob29HW1ub3iHp4pNPPsHSpUthtVr52ziAjo4OOBwOnDhxAm++\n+SbOnDkTsq2miVzOOnRKTGNjY6ioqMAzzzyDxx57TO9wYkJqaioeffRRfPttYhaNOnv2LFpbW7Fi\nxQpUVVXh1KlT2Lp1q95h6WbZsmUAgDvvvBOPP/645B87NU3kctahU+IRRRE7duxAXl4e6uvr9Q5H\nVy6XC0NDQwCAv//+G1988QWsVqvOUelj37596O/vx+XLl/HRRx+hpKRk4h2VRDM6OoqRkREAwI0b\nN/D5559jzZo1Idtrmsh916Hn5eXhqaeeQm5urpZDxqyqqirce++9+Omnn5CVlZXQ6+07Ojpw7Ngx\nnD59GlarFVarFYIg6B2WLq5evYqSkhJYLBbYbDaUlZXhwQcf1DusmJDIj2YHBwdRXFw8cV1s2rQJ\nDz/8cMj2mq8jJyIibfFPw0REcY6JnIgozjGRExHFOSZyIqI4x0RORBTnmMiJiOLc/wA+Si7R1Df5\nugAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1091abfd0>"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This report shows how IPython can be used to provide interactive introspection, monitoring and control capabilities to existing Python HPC codes that use MPI, requiring only trivial and non-intrusive modifications to the codes that don't introduce any permanent IPython dependency.  We have illustrated this by monitoring a shallow water simulation performed with the Proteus toolkit.\n",
      "\n",
      "The only assumption made was that the HPC code has a \"doit\" method or function that runs the main body of the code.  If the code has fine-grained time-stepping control, we can use IPython to manually move the time step forward, pause and restart the simulation, and modify intermediate variables.  But even if only a global \"do it all\" capability exists, we can still introspect and monitor the simulation's evolution, visualizing intermediate results as they are generated in the HPC nodes but using our local visualization capabilities.  This allows for human analysis of the simulation as the process is ongoing, without having to add visualization support to the main code or having to slow down the simulation for interactive introspection.\n",
      "\n",
      "We know of no other system that provides interactive monitoring and visualization of a traditional MPI code with such low development effort, changes to the existing code and performance impact."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}